  0%|                                                                                                                                                                                                                                            | 0/375 [00:00<?, ?it/s]
[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[21], line 1[0m
[0;32m----> 1[0m [43mtrainer[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[43m)[49m

File [0;32m<string>:157[0m, in [0;36mtrain[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m

File [0;32m<string>:380[0m, in [0;36m_fast_inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m

File [0;32m<string>:31[0m, in [0;36m_unsloth_training_step[0;34m(self, model, inputs, num_items_in_batch)[0m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1028[0m, in [0;36m_unsloth_pre_compute_loss[0;34m(self, model, inputs, *args, **kwargs)[0m
[1;32m   1026[0m     [38;5;28;01mpass[39;00m
[1;32m   1027[0m [38;5;28;01mpass[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_old_compute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/transformers/trainer.py:3721[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs, num_items_in_batch)[0m
[1;32m   3719[0m         loss_kwargs[[38;5;124m"[39m[38;5;124mnum_items_in_batch[39m[38;5;124m"[39m] [38;5;241m=[39m num_items_in_batch
[1;32m   3720[0m     inputs [38;5;241m=[39m {[38;5;241m*[39m[38;5;241m*[39minputs, [38;5;241m*[39m[38;5;241m*[39mloss_kwargs}
[0;32m-> 3721[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minputs[49m[43m)[49m
[1;32m   3722[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3723[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3724[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:1105[0m, in [0;36mPeftModelForCausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)[0m
[1;32m   1090[0m [38;5;129m@torch[39m[38;5;241m.[39m_disable_dynamo
[1;32m   1091[0m [38;5;28;01mdef[39;00m [38;5;21mPeftModelForCausalLM_fast_forward[39m(
[1;32m   1092[0m     [38;5;28mself[39m,
[0;32m   (...)[0m
[1;32m   1103[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   1104[0m ):
[0;32m-> 1105[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbase_model[49m[43m([49m
[1;32m   1106[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m   1107[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m   1108[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m   1109[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m   1110[0m [43m        [49m[43mlabels[49m[38;5;241;43m=[39;49m[43mlabels[49m[43m,[49m
[1;32m   1111[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m   1112[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m   1113[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m   1114[0m [43m        [49m[43mnum_logits_to_keep[49m[38;5;241;43m=[39;49m[43mnum_logits_to_keep[49m[43m,[49m
[1;32m   1115[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   1116[0m [43m    [49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197[0m, in [0;36mBaseTuner.forward[0;34m(self, *args, **kwargs)[0m
[1;32m    196[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;241m*[39margs: Any, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any):
[0;32m--> 197[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[38;5;241;43m.[39;49m[43mforward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:969[0m, in [0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)[0m
[1;32m    967[0m     [38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)[39;00m
[1;32m    968[0m     [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39m_has_no_labels [38;5;241m=[39m labels [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m
[0;32m--> 969[0m     outputs [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m
[1;32m    970[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m    971[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    972[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    973[0m [43m        [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    974[0m [43m        [49m[43mpast_key_values[49m[38;5;241;43m=[39;49m[43mpast_key_values[49m[43m,[49m
[1;32m    975[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m    976[0m [43m        [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    977[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    978[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m    979[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m    980[0m [43m    [49m[43m)[49m
[1;32m    981[0m [38;5;28;01mpass[39;00m
[1;32m    982[0m hidden_states [38;5;241m=[39m outputs[[38;5;241m0[39m]

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:820[0m, in [0;36mLlamaModel_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)[0m
[1;32m    817[0m         [38;5;28;01mreturn[39;00m custom_forward
[1;32m    818[0m     [38;5;28;01mpass[39;00m
[0;32m--> 820[0m     layer_outputs [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mutils[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[43m([49m
[1;32m    821[0m [43m        [49m[43mcreate_custom_forward[49m[43m([49m[43mdecoder_layer[49m[43m)[49m[43m,[49m
[1;32m    822[0m [43m        [49m[43mhidden_states[49m[43m,[49m
[1;32m    823[0m [43m        [49m[43mmask[49m[43m,[49m
[1;32m    824[0m [43m        [49m[43mattention_mask[49m[43m,[49m
[1;32m    825[0m [43m        [49m[43mposition_ids[49m[43m,[49m
[1;32m    826[0m [43m        [49m[43muse_reentrant[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m    827[0m [43m        [49m[43mpreserve_rng_state[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[1;32m    828[0m [43m    [49m[43m)[49m
[1;32m    829[0m     hidden_states [38;5;241m=[39m layer_outputs[[38;5;241m0[39m]
[1;32m    831[0m [38;5;28;01melse[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36[0m, in [0;36mwrap_inline.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     34[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     35[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m---> 36[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:487[0m, in [0;36mcheckpoint[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)[0m
[1;32m    482[0m     [38;5;28;01mif[39;00m context_fn [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m noop_context_fn [38;5;129;01mor[39;00m debug [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mFalse[39;00m:
[1;32m    483[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    484[0m             [38;5;124m"[39m[38;5;124mPassing `context_fn` or `debug` is only supported when [39m[38;5;124m"[39m
[1;32m    485[0m             [38;5;124m"[39m[38;5;124muse_reentrant=False.[39m[38;5;124m"[39m
[1;32m    486[0m         )
[0;32m--> 487[0m     [38;5;28;01mreturn[39;00m [43mCheckpointFunction[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mfunction[49m[43m,[49m[43m [49m[43mpreserve[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    488[0m [38;5;28;01melse[39;00m:
[1;32m    489[0m     gen [38;5;241m=[39m _checkpoint_without_reentrant_generator(
[1;32m    490[0m         function, preserve, context_fn, determinism_check, debug, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs
[1;32m    491[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598[0m, in [0;36mFunction.apply[0;34m(cls, *args, **kwargs)[0m
[1;32m    595[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39m_are_functorch_transforms_active():
[1;32m    596[0m     [38;5;66;03m# See NOTE: [functorch vjp and autograd interaction][39;00m
[1;32m    597[0m     args [38;5;241m=[39m _functorch[38;5;241m.[39mutils[38;5;241m.[39munwrap_dead_wrappers(args)
[0;32m--> 598[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m    600[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_setup_ctx_defined:
[1;32m    601[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    602[0m         [38;5;124m"[39m[38;5;124mIn order to use an autograd.Function with functorch transforms [39m[38;5;124m"[39m
[1;32m    603[0m         [38;5;124m"[39m[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context [39m[38;5;124m"[39m
[1;32m    604[0m         [38;5;124m"[39m[38;5;124mstaticmethod. For more details, please see [39m[38;5;124m"[39m
[1;32m    605[0m         [38;5;124m"[39m[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html[39m[38;5;124m"[39m
[1;32m    606[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262[0m, in [0;36mCheckpointFunction.forward[0;34m(ctx, run_function, preserve_rng_state, *args)[0m
[1;32m    259[0m ctx[38;5;241m.[39msave_for_backward([38;5;241m*[39mtensor_inputs)
[1;32m    261[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 262[0m     outputs [38;5;241m=[39m [43mrun_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    263[0m [38;5;28;01mreturn[39;00m outputs

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:816[0m, in [0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward[0;34m(*inputs)[0m
[1;32m    815[0m [38;5;28;01mdef[39;00m [38;5;21mcustom_forward[39m([38;5;241m*[39minputs):
[0;32m--> 816[0m     [38;5;28;01mreturn[39;00m [43mmodule[49m[43m([49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[43mpast_key_value[49m[43m,[49m[43m [49m[43moutput_attentions[49m[43m,[49m[43m [49m[43mpadding_mask[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mpadding_mask[49m[43m,[49m[43m [49m[43mposition_embeddings[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mposition_embeddings[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:493[0m, in [0;36mLlamaDecoderLayer_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    491[0m residual [38;5;241m=[39m hidden_states
[1;32m    492[0m hidden_states [38;5;241m=[39m fast_rms_layernorm([38;5;28mself[39m[38;5;241m.[39minput_layernorm, hidden_states)
[0;32m--> 493[0m hidden_states, self_attn_weights, present_key_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mself_attn[49m[43m([49m
[1;32m    494[0m [43m    [49m[43mhidden_states[49m[38;5;241;43m=[39;49m[43mhidden_states[49m[43m,[49m
[1;32m    495[0m [43m    [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    496[0m [43m    [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    497[0m [43m    [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    498[0m [43m    [49m[43mpast_key_value[49m[38;5;241;43m=[39;49m[43mpast_key_value[49m[43m,[49m
[1;32m    499[0m [43m    [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    500[0m [43m    [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    501[0m [43m    [49m[43mpadding_mask[49m[38;5;241;43m=[39;49m[43mpadding_mask[49m[43m,[49m
[1;32m    502[0m [43m[49m[43m)[49m
[1;32m    503[0m hidden_states [38;5;241m=[39m residual [38;5;241m+[39m hidden_states
[1;32m    505[0m [38;5;66;03m# Fully Connected[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:372[0m, in [0;36mLlamaAttention_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    369[0m     kv_seq_len [38;5;241m+[39m[38;5;241m=[39m past_key_value[[38;5;241m0[39m][38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m2[39m]
[1;32m    371[0m [38;5;66;03m# Extend RoPE dynamically to fit in VRAM[39;00m
[0;32m--> 372[0m rotary_emb [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrotary_emb[49m
[1;32m    373[0m rotary_emb[38;5;241m.[39mextend_rope_embedding(V, seq_len [38;5;241m=[39m kv_seq_len)
[1;32m    375[0m [38;5;28;01mif[39;00m position_ids [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    376[0m     [38;5;66;03m# Useful for LongRoPE[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaAttention' object has no attribute 'rotary_emb'


==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.48.0.dev0.
   \\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.54 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.3.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 2.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:11<00:00,  2.60s/it]
barc0/Llama-3.1-ARC-Potpourri-Induction-8B does not have a padding token! Will use pad_token = <|finetune_right_pad_id|>.


LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)



PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
   \\   /|    Num examples = 2,000 | Num Epochs = 3
O^O/ \_/ \    Batch size per device = 8 | Gradient Accumulation steps = 2
\        /    Total batch size = 16 | Total steps = 375
 "-____-"     Number of trainable parameters = 167,772,160

  0%|                                                                                                       | 0/375 [00:00<?, ?it/s]


[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[27], line 1[0m
[0;32m----> 1[0m [43mtrainer[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[43m)[49m

File [0;32m<string>:157[0m, in [0;36mtrain[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m

File [0;32m<string>:380[0m, in [0;36m_fast_inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m

File [0;32m<string>:31[0m, in [0;36m_unsloth_training_step[0;34m(self, model, inputs, num_items_in_batch)[0m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1028[0m, in [0;36m_unsloth_pre_compute_loss[0;34m(self, model, inputs, *args, **kwargs)[0m
[1;32m   1026[0m     [38;5;28;01mpass[39;00m
[1;32m   1027[0m [38;5;28;01mpass[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_old_compute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/transformers/trainer.py:3721[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs, num_items_in_batch)[0m
[1;32m   3719[0m         loss_kwargs[[38;5;124m"[39m[38;5;124mnum_items_in_batch[39m[38;5;124m"[39m] [38;5;241m=[39m num_items_in_batch
[1;32m   3720[0m     inputs [38;5;241m=[39m {[38;5;241m*[39m[38;5;241m*[39minputs, [38;5;241m*[39m[38;5;241m*[39mloss_kwargs}
[0;32m-> 3721[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minputs[49m[43m)[49m
[1;32m   3722[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3723[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3724[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:1105[0m, in [0;36mPeftModelForCausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)[0m
[1;32m   1090[0m [38;5;129m@torch[39m[38;5;241m.[39m_disable_dynamo
[1;32m   1091[0m [38;5;28;01mdef[39;00m [38;5;21mPeftModelForCausalLM_fast_forward[39m(
[1;32m   1092[0m     [38;5;28mself[39m,
[0;32m   (...)[0m
[1;32m   1103[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   1104[0m ):
[0;32m-> 1105[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbase_model[49m[43m([49m
[1;32m   1106[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m   1107[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m   1108[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m   1109[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m   1110[0m [43m        [49m[43mlabels[49m[38;5;241;43m=[39;49m[43mlabels[49m[43m,[49m
[1;32m   1111[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m   1112[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m   1113[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m   1114[0m [43m        [49m[43mnum_logits_to_keep[49m[38;5;241;43m=[39;49m[43mnum_logits_to_keep[49m[43m,[49m
[1;32m   1115[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   1116[0m [43m    [49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197[0m, in [0;36mBaseTuner.forward[0;34m(self, *args, **kwargs)[0m
[1;32m    196[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;241m*[39margs: Any, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any):
[0;32m--> 197[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[38;5;241;43m.[39;49m[43mforward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:969[0m, in [0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)[0m
[1;32m    967[0m     [38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)[39;00m
[1;32m    968[0m     [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39m_has_no_labels [38;5;241m=[39m labels [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m
[0;32m--> 969[0m     outputs [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m
[1;32m    970[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m    971[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    972[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    973[0m [43m        [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    974[0m [43m        [49m[43mpast_key_values[49m[38;5;241;43m=[39;49m[43mpast_key_values[49m[43m,[49m
[1;32m    975[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m    976[0m [43m        [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    977[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    978[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m    979[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m    980[0m [43m    [49m[43m)[49m
[1;32m    981[0m [38;5;28;01mpass[39;00m
[1;32m    982[0m hidden_states [38;5;241m=[39m outputs[[38;5;241m0[39m]

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:820[0m, in [0;36mLlamaModel_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)[0m
[1;32m    817[0m         [38;5;28;01mreturn[39;00m custom_forward
[1;32m    818[0m     [38;5;28;01mpass[39;00m
[0;32m--> 820[0m     layer_outputs [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mutils[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[43m([49m
[1;32m    821[0m [43m        [49m[43mcreate_custom_forward[49m[43m([49m[43mdecoder_layer[49m[43m)[49m[43m,[49m
[1;32m    822[0m [43m        [49m[43mhidden_states[49m[43m,[49m
[1;32m    823[0m [43m        [49m[43mmask[49m[43m,[49m
[1;32m    824[0m [43m        [49m[43mattention_mask[49m[43m,[49m
[1;32m    825[0m [43m        [49m[43mposition_ids[49m[43m,[49m
[1;32m    826[0m [43m        [49m[43muse_reentrant[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m    827[0m [43m        [49m[43mpreserve_rng_state[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[1;32m    828[0m [43m    [49m[43m)[49m
[1;32m    829[0m     hidden_states [38;5;241m=[39m layer_outputs[[38;5;241m0[39m]
[1;32m    831[0m [38;5;28;01melse[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36[0m, in [0;36mwrap_inline.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     34[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     35[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m---> 36[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:487[0m, in [0;36mcheckpoint[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)[0m
[1;32m    482[0m     [38;5;28;01mif[39;00m context_fn [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m noop_context_fn [38;5;129;01mor[39;00m debug [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mFalse[39;00m:
[1;32m    483[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    484[0m             [38;5;124m"[39m[38;5;124mPassing `context_fn` or `debug` is only supported when [39m[38;5;124m"[39m
[1;32m    485[0m             [38;5;124m"[39m[38;5;124muse_reentrant=False.[39m[38;5;124m"[39m
[1;32m    486[0m         )
[0;32m--> 487[0m     [38;5;28;01mreturn[39;00m [43mCheckpointFunction[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mfunction[49m[43m,[49m[43m [49m[43mpreserve[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    488[0m [38;5;28;01melse[39;00m:
[1;32m    489[0m     gen [38;5;241m=[39m _checkpoint_without_reentrant_generator(
[1;32m    490[0m         function, preserve, context_fn, determinism_check, debug, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs
[1;32m    491[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598[0m, in [0;36mFunction.apply[0;34m(cls, *args, **kwargs)[0m
[1;32m    595[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39m_are_functorch_transforms_active():
[1;32m    596[0m     [38;5;66;03m# See NOTE: [functorch vjp and autograd interaction][39;00m
[1;32m    597[0m     args [38;5;241m=[39m _functorch[38;5;241m.[39mutils[38;5;241m.[39munwrap_dead_wrappers(args)
[0;32m--> 598[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m    600[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_setup_ctx_defined:
[1;32m    601[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    602[0m         [38;5;124m"[39m[38;5;124mIn order to use an autograd.Function with functorch transforms [39m[38;5;124m"[39m
[1;32m    603[0m         [38;5;124m"[39m[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context [39m[38;5;124m"[39m
[1;32m    604[0m         [38;5;124m"[39m[38;5;124mstaticmethod. For more details, please see [39m[38;5;124m"[39m
[1;32m    605[0m         [38;5;124m"[39m[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html[39m[38;5;124m"[39m
[1;32m    606[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262[0m, in [0;36mCheckpointFunction.forward[0;34m(ctx, run_function, preserve_rng_state, *args)[0m
[1;32m    259[0m ctx[38;5;241m.[39msave_for_backward([38;5;241m*[39mtensor_inputs)
[1;32m    261[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 262[0m     outputs [38;5;241m=[39m [43mrun_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    263[0m [38;5;28;01mreturn[39;00m outputs

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:816[0m, in [0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward[0;34m(*inputs)[0m
[1;32m    815[0m [38;5;28;01mdef[39;00m [38;5;21mcustom_forward[39m([38;5;241m*[39minputs):
[0;32m--> 816[0m     [38;5;28;01mreturn[39;00m [43mmodule[49m[43m([49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[43mpast_key_value[49m[43m,[49m[43m [49m[43moutput_attentions[49m[43m,[49m[43m [49m[43mpadding_mask[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mpadding_mask[49m[43m,[49m[43m [49m[43mposition_embeddings[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mposition_embeddings[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:493[0m, in [0;36mLlamaDecoderLayer_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    491[0m residual [38;5;241m=[39m hidden_states
[1;32m    492[0m hidden_states [38;5;241m=[39m fast_rms_layernorm([38;5;28mself[39m[38;5;241m.[39minput_layernorm, hidden_states)
[0;32m--> 493[0m hidden_states, self_attn_weights, present_key_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mself_attn[49m[43m([49m
[1;32m    494[0m [43m    [49m[43mhidden_states[49m[38;5;241;43m=[39;49m[43mhidden_states[49m[43m,[49m
[1;32m    495[0m [43m    [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    496[0m [43m    [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    497[0m [43m    [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    498[0m [43m    [49m[43mpast_key_value[49m[38;5;241;43m=[39;49m[43mpast_key_value[49m[43m,[49m
[1;32m    499[0m [43m    [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    500[0m [43m    [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    501[0m [43m    [49m[43mpadding_mask[49m[38;5;241;43m=[39;49m[43mpadding_mask[49m[43m,[49m
[1;32m    502[0m [43m[49m[43m)[49m
[1;32m    503[0m hidden_states [38;5;241m=[39m residual [38;5;241m+[39m hidden_states
[1;32m    505[0m [38;5;66;03m# Fully Connected[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:372[0m, in [0;36mLlamaAttention_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    369[0m     kv_seq_len [38;5;241m+[39m[38;5;241m=[39m past_key_value[[38;5;241m0[39m][38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m2[39m]
[1;32m    371[0m [38;5;66;03m# Extend RoPE dynamically to fit in VRAM[39;00m
[0;32m--> 372[0m rotary_emb [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrotary_emb[49m
[1;32m    373[0m rotary_emb[38;5;241m.[39mextend_rope_embedding(V, seq_len [38;5;241m=[39m kv_seq_len)
[1;32m    375[0m [38;5;28;01mif[39;00m position_ids [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    376[0m     [38;5;66;03m# Useful for LongRoPE[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaAttention' object has no attribute 'rotary_emb'


PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)


[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn[29], line 1[0m
[0;32m----> 1[0m [43mmode[49m

[0;31mNameError[0m: name 'mode' is not defined



PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn[31], line 1[0m
[0;32m----> 1[0m [43mmdoel[49m[38;5;241m.[39mrotary_emb

[0;31mNameError[0m: name 'mdoel' is not defined

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/peft_model.py:824[0m, in [0;36mPeftModel.__getattr__[0;34m(self, name)[0m
[1;32m    823[0m [38;5;28;01mtry[39;00m:
[0;32m--> 824[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__getattr__[39;49m[43m([49m[43mname[49m[43m)[49m  [38;5;66;03m# defer to nn.Module's logic[39;00m
[1;32m    825[0m [38;5;28;01mexcept[39;00m [38;5;167;01mAttributeError[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'PeftModelForCausalLM' object has no attribute 'rotary_emb'

During handling of the above exception, another exception occurred:

[0;31mAttributeError[0m                            Traceback (most recent call last)
File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:371[0m, in [0;36mLoraModel.__getattr__[0;34m(self, name)[0m
[1;32m    370[0m [38;5;28;01mtry[39;00m:
[0;32m--> 371[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[38;5;21;43m__getattr__[39;49m[43m([49m[43mname[49m[43m)[49m  [38;5;66;03m# defer to nn.Module's logic[39;00m
[1;32m    372[0m [38;5;28;01mexcept[39;00m [38;5;167;01mAttributeError[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LoraModel' object has no attribute 'rotary_emb'

During handling of the above exception, another exception occurred:

[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[32], line 1[0m
[0;32m----> 1[0m [43mmodel[49m[38;5;241;43m.[39;49m[43mrotary_emb[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/peft_model.py:828[0m, in [0;36mPeftModel.__getattr__[0;34m(self, name)[0m
[1;32m    826[0m [38;5;28;01mif[39;00m name [38;5;241m==[39m [38;5;124m"[39m[38;5;124mbase_model[39m[38;5;124m"[39m:  [38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized[39;00m
[1;32m    827[0m     [38;5;28;01mraise[39;00m
[0;32m--> 828[0m [38;5;28;01mreturn[39;00m [38;5;28mgetattr[39m([38;5;28mself[39m[38;5;241m.[39mbase_model, name)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/lora/model.py:375[0m, in [0;36mLoraModel.__getattr__[0;34m(self, name)[0m
[1;32m    373[0m [38;5;28;01mif[39;00m name [38;5;241m==[39m [38;5;124m"[39m[38;5;124mmodel[39m[38;5;124m"[39m:  [38;5;66;03m# see #1892: prevent infinite recursion if class is not initialized[39;00m
[1;32m    374[0m     [38;5;28;01mraise[39;00m
[0;32m--> 375[0m [38;5;28;01mreturn[39;00m [38;5;28mgetattr[39m([38;5;28mself[39m[38;5;241m.[39mmodel, name)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaForCausalLM' object has no attribute 'rotary_emb'

<generator object Module.modules at 0x7a3586d75ee0>

<generator object Module.modules at 0x7a350c1db060>


[PeftModelForCausalLM(
   (base_model): LoraModel(
     (model): LlamaForCausalLM(
       (model): LlamaModel(
         (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
         (layers): ModuleList(
           (0-31): 32 x LlamaDecoderLayer(
             (self_attn): LlamaAttention(
               (q_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=4096, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (k_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=1024, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (v_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=1024, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (o_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=4096, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
             )
             (mlp): LlamaMLP(
               (gate_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=14336, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (up_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=4096, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=14336, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (down_proj): lora.Linear4bit(
                 (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                 (lora_dropout): ModuleDict(
                   (default): Identity()
                 )
                 (lora_A): ModuleDict(
                   (default): Linear(in_features=14336, out_features=64, bias=False)
                 )
                 (lora_B): ModuleDict(
                   (default): Linear(in_features=64, out_features=4096, bias=False)
                 )
                 (lora_embedding_A): ParameterDict()
                 (lora_embedding_B): ParameterDict()
                 (lora_magnitude_vector): ModuleDict()
               )
               (act_fn): SiLU()
             )
             (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
             (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
           )
         )
         (norm): LlamaRMSNorm((4096,), eps=1e-05)
         (rotary_emb): LlamaRotaryEmbedding()
       )
       (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
     )
   )
 ),
 LoraModel(
   (model): LlamaForCausalLM(
     (model): LlamaModel(
       (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
       (layers): ModuleList(
         (0-31): 32 x LlamaDecoderLayer(
           (self_attn): LlamaAttention(
             (q_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=4096, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (k_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=1024, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (v_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=1024, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (o_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=4096, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
           )
           (mlp): LlamaMLP(
             (gate_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=14336, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (up_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=4096, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=14336, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (down_proj): lora.Linear4bit(
               (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
               (lora_dropout): ModuleDict(
                 (default): Identity()
               )
               (lora_A): ModuleDict(
                 (default): Linear(in_features=14336, out_features=64, bias=False)
               )
               (lora_B): ModuleDict(
                 (default): Linear(in_features=64, out_features=4096, bias=False)
               )
               (lora_embedding_A): ParameterDict()
               (lora_embedding_B): ParameterDict()
               (lora_magnitude_vector): ModuleDict()
             )
             (act_fn): SiLU()
           )
           (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
           (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
         )
       )
       (norm): LlamaRMSNorm((4096,), eps=1e-05)
       (rotary_emb): LlamaRotaryEmbedding()
     )
     (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
   )
 ),
 LlamaForCausalLM(
   (model): LlamaModel(
     (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
     (layers): ModuleList(
       (0-31): 32 x LlamaDecoderLayer(
         (self_attn): LlamaAttention(
           (q_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=4096, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (k_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=1024, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (v_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=1024, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (o_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=4096, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
         )
         (mlp): LlamaMLP(
           (gate_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=14336, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (up_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=4096, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=14336, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (down_proj): lora.Linear4bit(
             (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
             (lora_dropout): ModuleDict(
               (default): Identity()
             )
             (lora_A): ModuleDict(
               (default): Linear(in_features=14336, out_features=64, bias=False)
             )
             (lora_B): ModuleDict(
               (default): Linear(in_features=64, out_features=4096, bias=False)
             )
             (lora_embedding_A): ParameterDict()
             (lora_embedding_B): ParameterDict()
             (lora_magnitude_vector): ModuleDict()
           )
           (act_fn): SiLU()
         )
         (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
         (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
       )
     )
     (norm): LlamaRMSNorm((4096,), eps=1e-05)
     (rotary_emb): LlamaRotaryEmbedding()
   )
   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
 ),
 LlamaModel(
   (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
   (layers): ModuleList(
     (0-31): 32 x LlamaDecoderLayer(
       (self_attn): LlamaAttention(
         (q_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=4096, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (k_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=1024, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (v_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=1024, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (o_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=4096, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
       )
       (mlp): LlamaMLP(
         (gate_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=14336, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (up_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=4096, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=14336, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (down_proj): lora.Linear4bit(
           (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
           (lora_dropout): ModuleDict(
             (default): Identity()
           )
           (lora_A): ModuleDict(
             (default): Linear(in_features=14336, out_features=64, bias=False)
           )
           (lora_B): ModuleDict(
             (default): Linear(in_features=64, out_features=4096, bias=False)
           )
           (lora_embedding_A): ParameterDict()
           (lora_embedding_B): ParameterDict()
           (lora_magnitude_vector): ModuleDict()
         )
         (act_fn): SiLU()
       )
       (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
       (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
     )
   )
   (norm): LlamaRMSNorm((4096,), eps=1e-05)
   (rotary_emb): LlamaRotaryEmbedding()
 ),
 Embedding(128256, 4096, padding_idx=128004),
 ModuleList(
   (0-31): 32 x LlamaDecoderLayer(
     (self_attn): LlamaAttention(
       (q_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=4096, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (k_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=1024, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (v_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=1024, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (o_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=4096, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
     )
     (mlp): LlamaMLP(
       (gate_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=14336, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (up_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=4096, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=14336, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (down_proj): lora.Linear4bit(
         (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
         (lora_dropout): ModuleDict(
           (default): Identity()
         )
         (lora_A): ModuleDict(
           (default): Linear(in_features=14336, out_features=64, bias=False)
         )
         (lora_B): ModuleDict(
           (default): Linear(in_features=64, out_features=4096, bias=False)
         )
         (lora_embedding_A): ParameterDict()
         (lora_embedding_B): ParameterDict()
         (lora_magnitude_vector): ModuleDict()
       )
       (act_fn): SiLU()
     )
     (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
     (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   )
 ),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaRMSNorm((4096,), eps=1e-05),
 LlamaDecoderLayer(
   (self_attn): LlamaAttention(
     (q_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (k_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (v_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=1024, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (o_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
   )
   (mlp): LlamaMLP(
     (gate_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (up_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=4096, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=14336, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (down_proj): lora.Linear4bit(
       (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
       (lora_dropout): ModuleDict(
         (default): Identity()
       )
       (lora_A): ModuleDict(
         (default): Linear(in_features=14336, out_features=64, bias=False)
       )
       (lora_B): ModuleDict(
         (default): Linear(in_features=64, out_features=4096, bias=False)
       )
       (lora_embedding_A): ParameterDict()
       (lora_embedding_B): ParameterDict()
       (lora_magnitude_vector): ModuleDict()
     )
     (act_fn): SiLU()
   )
   (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
   (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
 ),
 LlamaAttention(
   (q_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (k_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (v_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=1024, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (o_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=1024, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=1024, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=1024, bias=False)
 ),
 Linear(in_features=64, out_features=1024, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 LlamaMLP(
   (gate_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (up_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=4096, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=14336, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (down_proj): lora.Linear4bit(
     (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
     (lora_dropout): ModuleDict(
       (default): Identity()
     )
     (lora_A): ModuleDict(
       (default): Linear(in_features=14336, out_features=64, bias=False)
     )
     (lora_B): ModuleDict(
       (default): Linear(in_features=64, out_features=4096, bias=False)
     )
     (lora_embedding_A): ParameterDict()
     (lora_embedding_B): ParameterDict()
     (lora_magnitude_vector): ModuleDict()
   )
   (act_fn): SiLU()
 ),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=4096, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=14336, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=4096, out_features=14336, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=4096, out_features=64, bias=False)
 ),
 Linear(in_features=4096, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=14336, bias=False)
 ),
 Linear(in_features=64, out_features=14336, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 lora.Linear4bit(
   (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
   (lora_dropout): ModuleDict(
     (default): Identity()
   )
   (lora_A): ModuleDict(
     (default): Linear(in_features=14336, out_features=64, bias=False)
   )
   (lora_B): ModuleDict(
     (default): Linear(in_features=64, out_features=4096, bias=False)
   )
   (lora_embedding_A): ParameterDict()
   (lora_embedding_B): ParameterDict()
   (lora_magnitude_vector): ModuleDict()
 ),
 Linear4bit(in_features=14336, out_features=4096, bias=False),
 ModuleDict(
   (default): Identity()
 ),
 Identity(),
 ModuleDict(
   (default): Linear(in_features=14336, out_features=64, bias=False)
 ),
 Linear(in_features=14336, out_features=64, bias=False),
 ModuleDict(
   (default): Linear(in_features=64, out_features=4096, bias=False)
 ),
 Linear(in_features=64, out_features=4096, bias=False),
 ParameterDict(),
 ParameterDict(),
 ModuleDict(),
 SiLU(),
 ...]
  0%|                                                                                                       | 0/375 [01:53<?, ?it/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 2,000 | Num Epochs = 3
O^O/ \_/ \    Batch size per device = 8 | Gradient Accumulation steps = 2
\        /    Total batch size = 16 | Total steps = 375
 "-____-"     Number of trainable parameters = 167,772,160
  0%|                                                                                                       | 0/375 [00:00<?, ?it/s]




[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[37], line 1[0m
[0;32m----> 1[0m [43mtrainer[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[43m)[49m

File [0;32m<string>:157[0m, in [0;36mtrain[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m

File [0;32m<string>:380[0m, in [0;36m_fast_inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m

File [0;32m<string>:31[0m, in [0;36m_unsloth_training_step[0;34m(self, model, inputs, num_items_in_batch)[0m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1028[0m, in [0;36m_unsloth_pre_compute_loss[0;34m(self, model, inputs, *args, **kwargs)[0m
[1;32m   1026[0m     [38;5;28;01mpass[39;00m
[1;32m   1027[0m [38;5;28;01mpass[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_old_compute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/transformers/trainer.py:3721[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs, num_items_in_batch)[0m
[1;32m   3719[0m         loss_kwargs[[38;5;124m"[39m[38;5;124mnum_items_in_batch[39m[38;5;124m"[39m] [38;5;241m=[39m num_items_in_batch
[1;32m   3720[0m     inputs [38;5;241m=[39m {[38;5;241m*[39m[38;5;241m*[39minputs, [38;5;241m*[39m[38;5;241m*[39mloss_kwargs}
[0;32m-> 3721[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minputs[49m[43m)[49m
[1;32m   3722[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3723[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3724[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:1105[0m, in [0;36mPeftModelForCausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)[0m
[1;32m   1090[0m [38;5;129m@torch[39m[38;5;241m.[39m_disable_dynamo
[1;32m   1091[0m [38;5;28;01mdef[39;00m [38;5;21mPeftModelForCausalLM_fast_forward[39m(
[1;32m   1092[0m     [38;5;28mself[39m,
[0;32m   (...)[0m
[1;32m   1103[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   1104[0m ):
[0;32m-> 1105[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbase_model[49m[43m([49m
[1;32m   1106[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m   1107[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m   1108[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m   1109[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m   1110[0m [43m        [49m[43mlabels[49m[38;5;241;43m=[39;49m[43mlabels[49m[43m,[49m
[1;32m   1111[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m   1112[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m   1113[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m   1114[0m [43m        [49m[43mnum_logits_to_keep[49m[38;5;241;43m=[39;49m[43mnum_logits_to_keep[49m[43m,[49m
[1;32m   1115[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   1116[0m [43m    [49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197[0m, in [0;36mBaseTuner.forward[0;34m(self, *args, **kwargs)[0m
[1;32m    196[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;241m*[39margs: Any, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any):
[0;32m--> 197[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[38;5;241;43m.[39;49m[43mforward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:969[0m, in [0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)[0m
[1;32m    967[0m     [38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)[39;00m
[1;32m    968[0m     [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39m_has_no_labels [38;5;241m=[39m labels [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m
[0;32m--> 969[0m     outputs [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m
[1;32m    970[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m    971[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    972[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    973[0m [43m        [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    974[0m [43m        [49m[43mpast_key_values[49m[38;5;241;43m=[39;49m[43mpast_key_values[49m[43m,[49m
[1;32m    975[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m    976[0m [43m        [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    977[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    978[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m    979[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m    980[0m [43m    [49m[43m)[49m
[1;32m    981[0m [38;5;28;01mpass[39;00m
[1;32m    982[0m hidden_states [38;5;241m=[39m outputs[[38;5;241m0[39m]

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:820[0m, in [0;36mLlamaModel_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)[0m
[1;32m    817[0m         [38;5;28;01mreturn[39;00m custom_forward
[1;32m    818[0m     [38;5;28;01mpass[39;00m
[0;32m--> 820[0m     layer_outputs [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mutils[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[43m([49m
[1;32m    821[0m [43m        [49m[43mcreate_custom_forward[49m[43m([49m[43mdecoder_layer[49m[43m)[49m[43m,[49m
[1;32m    822[0m [43m        [49m[43mhidden_states[49m[43m,[49m
[1;32m    823[0m [43m        [49m[43mmask[49m[43m,[49m
[1;32m    824[0m [43m        [49m[43mattention_mask[49m[43m,[49m
[1;32m    825[0m [43m        [49m[43mposition_ids[49m[43m,[49m
[1;32m    826[0m [43m        [49m[43muse_reentrant[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m    827[0m [43m        [49m[43mpreserve_rng_state[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[1;32m    828[0m [43m    [49m[43m)[49m
[1;32m    829[0m     hidden_states [38;5;241m=[39m layer_outputs[[38;5;241m0[39m]
[1;32m    831[0m [38;5;28;01melse[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36[0m, in [0;36mwrap_inline.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     34[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     35[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m---> 36[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:487[0m, in [0;36mcheckpoint[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)[0m
[1;32m    482[0m     [38;5;28;01mif[39;00m context_fn [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m noop_context_fn [38;5;129;01mor[39;00m debug [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mFalse[39;00m:
[1;32m    483[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    484[0m             [38;5;124m"[39m[38;5;124mPassing `context_fn` or `debug` is only supported when [39m[38;5;124m"[39m
[1;32m    485[0m             [38;5;124m"[39m[38;5;124muse_reentrant=False.[39m[38;5;124m"[39m
[1;32m    486[0m         )
[0;32m--> 487[0m     [38;5;28;01mreturn[39;00m [43mCheckpointFunction[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mfunction[49m[43m,[49m[43m [49m[43mpreserve[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    488[0m [38;5;28;01melse[39;00m:
[1;32m    489[0m     gen [38;5;241m=[39m _checkpoint_without_reentrant_generator(
[1;32m    490[0m         function, preserve, context_fn, determinism_check, debug, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs
[1;32m    491[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598[0m, in [0;36mFunction.apply[0;34m(cls, *args, **kwargs)[0m
[1;32m    595[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39m_are_functorch_transforms_active():
[1;32m    596[0m     [38;5;66;03m# See NOTE: [functorch vjp and autograd interaction][39;00m
[1;32m    597[0m     args [38;5;241m=[39m _functorch[38;5;241m.[39mutils[38;5;241m.[39munwrap_dead_wrappers(args)
[0;32m--> 598[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m    600[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_setup_ctx_defined:
[1;32m    601[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    602[0m         [38;5;124m"[39m[38;5;124mIn order to use an autograd.Function with functorch transforms [39m[38;5;124m"[39m
[1;32m    603[0m         [38;5;124m"[39m[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context [39m[38;5;124m"[39m
[1;32m    604[0m         [38;5;124m"[39m[38;5;124mstaticmethod. For more details, please see [39m[38;5;124m"[39m
[1;32m    605[0m         [38;5;124m"[39m[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html[39m[38;5;124m"[39m
[1;32m    606[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262[0m, in [0;36mCheckpointFunction.forward[0;34m(ctx, run_function, preserve_rng_state, *args)[0m
[1;32m    259[0m ctx[38;5;241m.[39msave_for_backward([38;5;241m*[39mtensor_inputs)
[1;32m    261[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 262[0m     outputs [38;5;241m=[39m [43mrun_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    263[0m [38;5;28;01mreturn[39;00m outputs

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:816[0m, in [0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward[0;34m(*inputs)[0m
[1;32m    815[0m [38;5;28;01mdef[39;00m [38;5;21mcustom_forward[39m([38;5;241m*[39minputs):
[0;32m--> 816[0m     [38;5;28;01mreturn[39;00m [43mmodule[49m[43m([49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[43mpast_key_value[49m[43m,[49m[43m [49m[43moutput_attentions[49m[43m,[49m[43m [49m[43mpadding_mask[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mpadding_mask[49m[43m,[49m[43m [49m[43mposition_embeddings[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mposition_embeddings[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:493[0m, in [0;36mLlamaDecoderLayer_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    491[0m residual [38;5;241m=[39m hidden_states
[1;32m    492[0m hidden_states [38;5;241m=[39m fast_rms_layernorm([38;5;28mself[39m[38;5;241m.[39minput_layernorm, hidden_states)
[0;32m--> 493[0m hidden_states, self_attn_weights, present_key_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mself_attn[49m[43m([49m
[1;32m    494[0m [43m    [49m[43mhidden_states[49m[38;5;241;43m=[39;49m[43mhidden_states[49m[43m,[49m
[1;32m    495[0m [43m    [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    496[0m [43m    [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    497[0m [43m    [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    498[0m [43m    [49m[43mpast_key_value[49m[38;5;241;43m=[39;49m[43mpast_key_value[49m[43m,[49m
[1;32m    499[0m [43m    [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    500[0m [43m    [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    501[0m [43m    [49m[43mpadding_mask[49m[38;5;241;43m=[39;49m[43mpadding_mask[49m[43m,[49m
[1;32m    502[0m [43m[49m[43m)[49m
[1;32m    503[0m hidden_states [38;5;241m=[39m residual [38;5;241m+[39m hidden_states
[1;32m    505[0m [38;5;66;03m# Fully Connected[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:372[0m, in [0;36mLlamaAttention_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    369[0m     kv_seq_len [38;5;241m+[39m[38;5;241m=[39m past_key_value[[38;5;241m0[39m][38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m2[39m]
[1;32m    371[0m [38;5;66;03m# Extend RoPE dynamically to fit in VRAM[39;00m
[0;32m--> 372[0m rotary_emb [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrotary_emb[49m
[1;32m    373[0m rotary_emb[38;5;241m.[39mextend_rope_embedding(V, seq_len [38;5;241m=[39m kv_seq_len)
[1;32m    375[0m [38;5;28;01mif[39;00m position_ids [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    376[0m     [38;5;66;03m# Useful for LongRoPE[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaAttention' object has no attribute 'rotary_emb'

[0;31m---------------------------------------------------------------------------[0m
[0;31mNameError[0m                                 Traceback (most recent call last)
Cell [0;32mIn[38], line 6[0m
[1;32m      1[0m model, tokenizer [38;5;241m=[39m FastLanguageModel[38;5;241m.[39mfrom_pretrained(
[1;32m      2[0m     [38;5;66;03m# "barc0/Llama-3.1-ARC-Potpourri-Induction-8B",[39;00m
[1;32m      3[0m     [38;5;66;03m# dtype=torch.bfloat16,[39;00m
[1;32m      4[0m     [38;5;124m"[39m[38;5;124munsloth/Meta-Llama-3.1-8B-Instruct[39m[38;5;124m"[39m,
[1;32m      5[0m     dtype[38;5;241m=[39mtorch[38;5;241m.[39mbfloat16,
[0;32m----> 6[0m     token[38;5;241m=[39m[43msettings[49m[38;5;241m.[39mHF_API_TOKEN,
[1;32m      7[0m )
[1;32m      8[0m model [38;5;241m=[39m FastLanguageModel[38;5;241m.[39mget_peft_model(
[1;32m      9[0m     model,
[1;32m     10[0m     r[38;5;241m=[39m[38;5;241m64[39m,
[0;32m   (...)[0m
[1;32m     25[0m     random_state[38;5;241m=[39m[38;5;241m3407[39m,
[1;32m     26[0m )

[0;31mNameError[0m: name 'settings' is not defined


==((====))==  Unsloth 2024.12.4: Fast Llama patching. Transformers:4.48.0.dev0.
   \\   /|    GPU: NVIDIA GeForce RTX 4090. Max memory: 23.54 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.3.1+cu121. CUDA: 8.9. CUDA Toolkit: 12.1. Triton: 2.3.1
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.27. FA2 = False]
 "-____-"     Free Apache license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
  0%|                                                                                                       | 0/375 [02:03<?, ?it/s]
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1
   \\   /|    Num examples = 2,000 | Num Epochs = 3
O^O/ \_/ \    Batch size per device = 8 | Gradient Accumulation steps = 2
\        /    Total batch size = 16 | Total steps = 375
 "-____-"     Number of trainable parameters = 167,772,160
  0%|                                                                                                       | 0/375 [00:00<?, ?it/s]

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[41], line 1[0m
[0;32m----> 1[0m [43mtrainer[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[43m)[49m

File [0;32m<string>:157[0m, in [0;36mtrain[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m

File [0;32m<string>:380[0m, in [0;36m_fast_inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m

File [0;32m<string>:31[0m, in [0;36m_unsloth_training_step[0;34m(self, model, inputs, num_items_in_batch)[0m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1028[0m, in [0;36m_unsloth_pre_compute_loss[0;34m(self, model, inputs, *args, **kwargs)[0m
[1;32m   1026[0m     [38;5;28;01mpass[39;00m
[1;32m   1027[0m [38;5;28;01mpass[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_old_compute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/transformers/trainer.py:3721[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs, num_items_in_batch)[0m
[1;32m   3719[0m         loss_kwargs[[38;5;124m"[39m[38;5;124mnum_items_in_batch[39m[38;5;124m"[39m] [38;5;241m=[39m num_items_in_batch
[1;32m   3720[0m     inputs [38;5;241m=[39m {[38;5;241m*[39m[38;5;241m*[39minputs, [38;5;241m*[39m[38;5;241m*[39mloss_kwargs}
[0;32m-> 3721[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minputs[49m[43m)[49m
[1;32m   3722[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3723[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3724[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:1105[0m, in [0;36mPeftModelForCausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)[0m
[1;32m   1090[0m [38;5;129m@torch[39m[38;5;241m.[39m_disable_dynamo
[1;32m   1091[0m [38;5;28;01mdef[39;00m [38;5;21mPeftModelForCausalLM_fast_forward[39m(
[1;32m   1092[0m     [38;5;28mself[39m,
[0;32m   (...)[0m
[1;32m   1103[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   1104[0m ):
[0;32m-> 1105[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbase_model[49m[43m([49m
[1;32m   1106[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m   1107[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m   1108[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m   1109[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m   1110[0m [43m        [49m[43mlabels[49m[38;5;241;43m=[39;49m[43mlabels[49m[43m,[49m
[1;32m   1111[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m   1112[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m   1113[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m   1114[0m [43m        [49m[43mnum_logits_to_keep[49m[38;5;241;43m=[39;49m[43mnum_logits_to_keep[49m[43m,[49m
[1;32m   1115[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   1116[0m [43m    [49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197[0m, in [0;36mBaseTuner.forward[0;34m(self, *args, **kwargs)[0m
[1;32m    196[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;241m*[39margs: Any, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any):
[0;32m--> 197[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[38;5;241;43m.[39;49m[43mforward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:969[0m, in [0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)[0m
[1;32m    967[0m     [38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)[39;00m
[1;32m    968[0m     [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39m_has_no_labels [38;5;241m=[39m labels [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m
[0;32m--> 969[0m     outputs [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m
[1;32m    970[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m    971[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    972[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    973[0m [43m        [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    974[0m [43m        [49m[43mpast_key_values[49m[38;5;241;43m=[39;49m[43mpast_key_values[49m[43m,[49m
[1;32m    975[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m    976[0m [43m        [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    977[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    978[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m    979[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m    980[0m [43m    [49m[43m)[49m
[1;32m    981[0m [38;5;28;01mpass[39;00m
[1;32m    982[0m hidden_states [38;5;241m=[39m outputs[[38;5;241m0[39m]

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:820[0m, in [0;36mLlamaModel_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)[0m
[1;32m    817[0m         [38;5;28;01mreturn[39;00m custom_forward
[1;32m    818[0m     [38;5;28;01mpass[39;00m
[0;32m--> 820[0m     layer_outputs [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mutils[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[43m([49m
[1;32m    821[0m [43m        [49m[43mcreate_custom_forward[49m[43m([49m[43mdecoder_layer[49m[43m)[49m[43m,[49m
[1;32m    822[0m [43m        [49m[43mhidden_states[49m[43m,[49m
[1;32m    823[0m [43m        [49m[43mmask[49m[43m,[49m
[1;32m    824[0m [43m        [49m[43mattention_mask[49m[43m,[49m
[1;32m    825[0m [43m        [49m[43mposition_ids[49m[43m,[49m
[1;32m    826[0m [43m        [49m[43muse_reentrant[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m    827[0m [43m        [49m[43mpreserve_rng_state[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[1;32m    828[0m [43m    [49m[43m)[49m
[1;32m    829[0m     hidden_states [38;5;241m=[39m layer_outputs[[38;5;241m0[39m]
[1;32m    831[0m [38;5;28;01melse[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36[0m, in [0;36mwrap_inline.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     34[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     35[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m---> 36[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:487[0m, in [0;36mcheckpoint[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)[0m
[1;32m    482[0m     [38;5;28;01mif[39;00m context_fn [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m noop_context_fn [38;5;129;01mor[39;00m debug [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mFalse[39;00m:
[1;32m    483[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    484[0m             [38;5;124m"[39m[38;5;124mPassing `context_fn` or `debug` is only supported when [39m[38;5;124m"[39m
[1;32m    485[0m             [38;5;124m"[39m[38;5;124muse_reentrant=False.[39m[38;5;124m"[39m
[1;32m    486[0m         )
[0;32m--> 487[0m     [38;5;28;01mreturn[39;00m [43mCheckpointFunction[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mfunction[49m[43m,[49m[43m [49m[43mpreserve[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    488[0m [38;5;28;01melse[39;00m:
[1;32m    489[0m     gen [38;5;241m=[39m _checkpoint_without_reentrant_generator(
[1;32m    490[0m         function, preserve, context_fn, determinism_check, debug, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs
[1;32m    491[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598[0m, in [0;36mFunction.apply[0;34m(cls, *args, **kwargs)[0m
[1;32m    595[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39m_are_functorch_transforms_active():
[1;32m    596[0m     [38;5;66;03m# See NOTE: [functorch vjp and autograd interaction][39;00m
[1;32m    597[0m     args [38;5;241m=[39m _functorch[38;5;241m.[39mutils[38;5;241m.[39munwrap_dead_wrappers(args)
[0;32m--> 598[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m    600[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_setup_ctx_defined:
[1;32m    601[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    602[0m         [38;5;124m"[39m[38;5;124mIn order to use an autograd.Function with functorch transforms [39m[38;5;124m"[39m
[1;32m    603[0m         [38;5;124m"[39m[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context [39m[38;5;124m"[39m
[1;32m    604[0m         [38;5;124m"[39m[38;5;124mstaticmethod. For more details, please see [39m[38;5;124m"[39m
[1;32m    605[0m         [38;5;124m"[39m[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html[39m[38;5;124m"[39m
[1;32m    606[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262[0m, in [0;36mCheckpointFunction.forward[0;34m(ctx, run_function, preserve_rng_state, *args)[0m
[1;32m    259[0m ctx[38;5;241m.[39msave_for_backward([38;5;241m*[39mtensor_inputs)
[1;32m    261[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 262[0m     outputs [38;5;241m=[39m [43mrun_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    263[0m [38;5;28;01mreturn[39;00m outputs

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:816[0m, in [0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward[0;34m(*inputs)[0m
[1;32m    815[0m [38;5;28;01mdef[39;00m [38;5;21mcustom_forward[39m([38;5;241m*[39minputs):
[0;32m--> 816[0m     [38;5;28;01mreturn[39;00m [43mmodule[49m[43m([49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[43mpast_key_value[49m[43m,[49m[43m [49m[43moutput_attentions[49m[43m,[49m[43m [49m[43mpadding_mask[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mpadding_mask[49m[43m,[49m[43m [49m[43mposition_embeddings[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mposition_embeddings[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:493[0m, in [0;36mLlamaDecoderLayer_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    491[0m residual [38;5;241m=[39m hidden_states
[1;32m    492[0m hidden_states [38;5;241m=[39m fast_rms_layernorm([38;5;28mself[39m[38;5;241m.[39minput_layernorm, hidden_states)
[0;32m--> 493[0m hidden_states, self_attn_weights, present_key_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mself_attn[49m[43m([49m
[1;32m    494[0m [43m    [49m[43mhidden_states[49m[38;5;241;43m=[39;49m[43mhidden_states[49m[43m,[49m
[1;32m    495[0m [43m    [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    496[0m [43m    [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    497[0m [43m    [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    498[0m [43m    [49m[43mpast_key_value[49m[38;5;241;43m=[39;49m[43mpast_key_value[49m[43m,[49m
[1;32m    499[0m [43m    [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    500[0m [43m    [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    501[0m [43m    [49m[43mpadding_mask[49m[38;5;241;43m=[39;49m[43mpadding_mask[49m[43m,[49m
[1;32m    502[0m [43m[49m[43m)[49m
[1;32m    503[0m hidden_states [38;5;241m=[39m residual [38;5;241m+[39m hidden_states
[1;32m    505[0m [38;5;66;03m# Fully Connected[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:372[0m, in [0;36mLlamaAttention_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    369[0m     kv_seq_len [38;5;241m+[39m[38;5;241m=[39m past_key_value[[38;5;241m0[39m][38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m2[39m]
[1;32m    371[0m [38;5;66;03m# Extend RoPE dynamically to fit in VRAM[39;00m
[0;32m--> 372[0m rotary_emb [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrotary_emb[49m
[1;32m    373[0m rotary_emb[38;5;241m.[39mextend_rope_embedding(V, seq_len [38;5;241m=[39m kv_seq_len)
[1;32m    375[0m [38;5;28;01mif[39;00m position_ids [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    376[0m     [38;5;66;03m# Useful for LongRoPE[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaAttention' object has no attribute 'rotary_emb'
==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1████████████████████████| 2000/2000 [00:04<00:00, 429.44 examples/s]
   \\   /|    Num examples = 2,000 | Num Epochs = 3
O^O/ \_/ \    Batch size per device = 8 | Gradient Accumulation steps = 2
\        /    Total batch size = 16 | Total steps = 375
 "-____-"     Number of trainable parameters = 167,772,160

  0%|                                                                                                       | 0/375 [00:00<?, ?it/s]

[0;31m---------------------------------------------------------------------------[0m
[0;31mAttributeError[0m                            Traceback (most recent call last)
Cell [0;32mIn[42], line 25[0m
[1;32m      1[0m trainer [38;5;241m=[39m SFTTrainer(
[1;32m      2[0m     model[38;5;241m=[39mmodel,
[1;32m      3[0m     tokenizer[38;5;241m=[39mtokenizer,
[0;32m   (...)[0m
[1;32m     22[0m     ),
[1;32m     23[0m )
[0;32m---> 25[0m [43mtrainer[49m[38;5;241;43m.[39;49m[43mtrain[49m[43m([49m[43m)[49m

File [0;32m<string>:157[0m, in [0;36mtrain[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)[0m

File [0;32m<string>:380[0m, in [0;36m_fast_inner_training_loop[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)[0m

File [0;32m<string>:31[0m, in [0;36m_unsloth_training_step[0;34m(self, model, inputs, num_items_in_batch)[0m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/_utils.py:1028[0m, in [0;36m_unsloth_pre_compute_loss[0;34m(self, model, inputs, *args, **kwargs)[0m
[1;32m   1026[0m     [38;5;28;01mpass[39;00m
[1;32m   1027[0m [38;5;28;01mpass[39;00m
[0;32m-> 1028[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_old_compute_loss[49m[43m([49m[43mmodel[49m[43m,[49m[43m [49m[43minputs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/transformers/trainer.py:3721[0m, in [0;36mTrainer.compute_loss[0;34m(self, model, inputs, return_outputs, num_items_in_batch)[0m
[1;32m   3719[0m         loss_kwargs[[38;5;124m"[39m[38;5;124mnum_items_in_batch[39m[38;5;124m"[39m] [38;5;241m=[39m num_items_in_batch
[1;32m   3720[0m     inputs [38;5;241m=[39m {[38;5;241m*[39m[38;5;241m*[39minputs, [38;5;241m*[39m[38;5;241m*[39mloss_kwargs}
[0;32m-> 3721[0m outputs [38;5;241m=[39m [43mmodel[49m[43m([49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43minputs[49m[43m)[49m
[1;32m   3722[0m [38;5;66;03m# Save past state if it exists[39;00m
[1;32m   3723[0m [38;5;66;03m# TODO: this needs to be fixed and made cleaner later.[39;00m
[1;32m   3724[0m [38;5;28;01mif[39;00m [38;5;28mself[39m[38;5;241m.[39margs[38;5;241m.[39mpast_index [38;5;241m>[39m[38;5;241m=[39m [38;5;241m0[39m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:823[0m, in [0;36mconvert_outputs_to_fp32.<locals>.forward[0;34m(*args, **kwargs)[0m
[1;32m    822[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 823[0m     [38;5;28;01mreturn[39;00m [43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/accelerate/utils/operations.py:811[0m, in [0;36mConvertOutputsToFp32.__call__[0;34m(self, *args, **kwargs)[0m
[1;32m    810[0m [38;5;28;01mdef[39;00m [38;5;21m__call__[39m([38;5;28mself[39m, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m--> 811[0m     [38;5;28;01mreturn[39;00m convert_to_fp32([38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel_forward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/amp/autocast_mode.py:16[0m, in [0;36mautocast_decorator.<locals>.decorate_autocast[0;34m(*args, **kwargs)[0m
[1;32m     13[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(func)
[1;32m     14[0m [38;5;28;01mdef[39;00m [38;5;21mdecorate_autocast[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     15[0m     [38;5;28;01mwith[39;00m autocast_instance:
[0;32m---> 16[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:1105[0m, in [0;36mPeftModelForCausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, **kwargs)[0m
[1;32m   1090[0m [38;5;129m@torch[39m[38;5;241m.[39m_disable_dynamo
[1;32m   1091[0m [38;5;28;01mdef[39;00m [38;5;21mPeftModelForCausalLM_fast_forward[39m(
[1;32m   1092[0m     [38;5;28mself[39m,
[0;32m   (...)[0m
[1;32m   1103[0m     [38;5;241m*[39m[38;5;241m*[39mkwargs,
[1;32m   1104[0m ):
[0;32m-> 1105[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mbase_model[49m[43m([49m
[1;32m   1106[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m   1107[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m   1108[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m   1109[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m   1110[0m [43m        [49m[43mlabels[49m[38;5;241;43m=[39;49m[43mlabels[49m[43m,[49m
[1;32m   1111[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m   1112[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m   1113[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m   1114[0m [43m        [49m[43mnum_logits_to_keep[49m[38;5;241;43m=[39;49m[43mnum_logits_to_keep[49m[43m,[49m
[1;32m   1115[0m [43m        [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m,[49m
[1;32m   1116[0m [43m    [49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/peft/tuners/tuners_utils.py:197[0m, in [0;36mBaseTuner.forward[0;34m(self, *args, **kwargs)[0m
[1;32m    196[0m [38;5;28;01mdef[39;00m [38;5;21mforward[39m([38;5;28mself[39m, [38;5;241m*[39margs: Any, [38;5;241m*[39m[38;5;241m*[39mkwargs: Any):
[0;32m--> 197[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[38;5;241;43m.[39;49m[43mforward[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:969[0m, in [0;36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, *args, **kwargs)[0m
[1;32m    967[0m     [38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)[39;00m
[1;32m    968[0m     [38;5;28mself[39m[38;5;241m.[39mmodel[38;5;241m.[39m_has_no_labels [38;5;241m=[39m labels [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m
[0;32m--> 969[0m     outputs [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mmodel[49m[43m([49m
[1;32m    970[0m [43m        [49m[43minput_ids[49m[38;5;241;43m=[39;49m[43minput_ids[49m[43m,[49m
[1;32m    971[0m [43m        [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    972[0m [43m        [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    973[0m [43m        [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    974[0m [43m        [49m[43mpast_key_values[49m[38;5;241;43m=[39;49m[43mpast_key_values[49m[43m,[49m
[1;32m    975[0m [43m        [49m[43minputs_embeds[49m[38;5;241;43m=[39;49m[43minputs_embeds[49m[43m,[49m
[1;32m    976[0m [43m        [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    977[0m [43m        [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    978[0m [43m        [49m[43moutput_hidden_states[49m[38;5;241;43m=[39;49m[43moutput_hidden_states[49m[43m,[49m
[1;32m    979[0m [43m        [49m[43mreturn_dict[49m[38;5;241;43m=[39;49m[43mreturn_dict[49m[43m,[49m
[1;32m    980[0m [43m    [49m[43m)[49m
[1;32m    981[0m [38;5;28;01mpass[39;00m
[1;32m    982[0m hidden_states [38;5;241m=[39m outputs[[38;5;241m0[39m]

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:820[0m, in [0;36mLlamaModel_fast_forward[0;34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)[0m
[1;32m    817[0m         [38;5;28;01mreturn[39;00m custom_forward
[1;32m    818[0m     [38;5;28;01mpass[39;00m
[0;32m--> 820[0m     layer_outputs [38;5;241m=[39m [43mtorch[49m[38;5;241;43m.[39;49m[43mutils[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[38;5;241;43m.[39;49m[43mcheckpoint[49m[43m([49m
[1;32m    821[0m [43m        [49m[43mcreate_custom_forward[49m[43m([49m[43mdecoder_layer[49m[43m)[49m[43m,[49m
[1;32m    822[0m [43m        [49m[43mhidden_states[49m[43m,[49m
[1;32m    823[0m [43m        [49m[43mmask[49m[43m,[49m
[1;32m    824[0m [43m        [49m[43mattention_mask[49m[43m,[49m
[1;32m    825[0m [43m        [49m[43mposition_ids[49m[43m,[49m
[1;32m    826[0m [43m        [49m[43muse_reentrant[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mTrue[39;49;00m[43m,[49m
[1;32m    827[0m [43m        [49m[43mpreserve_rng_state[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[38;5;28;43;01mFalse[39;49;00m[43m,[49m
[1;32m    828[0m [43m    [49m[43m)[49m
[1;32m    829[0m     hidden_states [38;5;241m=[39m layer_outputs[[38;5;241m0[39m]
[1;32m    831[0m [38;5;28;01melse[39;00m:

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_compile.py:24[0m, in [0;36m_disable_dynamo.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     20[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     21[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[1;32m     22[0m     [38;5;28;01mimport[39;00m [38;5;21;01mtorch[39;00m[38;5;21;01m.[39;00m[38;5;21;01m_dynamo[39;00m
[0;32m---> 24[0m     [38;5;28;01mreturn[39;00m [43mtorch[49m[38;5;241;43m.[39;49m[43m_dynamo[49m[38;5;241;43m.[39;49m[43mdisable[49m[43m([49m[43mfn[49m[43m,[49m[43m [49m[43mrecursive[49m[43m)[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:451[0m, in [0;36m_TorchDynamoContext.__call__.<locals>._fn[0;34m(*args, **kwargs)[0m
[1;32m    449[0m prior [38;5;241m=[39m set_eval_frame(callback)
[1;32m    450[0m [38;5;28;01mtry[39;00m:
[0;32m--> 451[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    452[0m [38;5;28;01mfinally[39;00m:
[1;32m    453[0m     set_eval_frame(prior)

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/_dynamo/external_utils.py:36[0m, in [0;36mwrap_inline.<locals>.inner[0;34m(*args, **kwargs)[0m
[1;32m     34[0m [38;5;129m@functools[39m[38;5;241m.[39mwraps(fn)
[1;32m     35[0m [38;5;28;01mdef[39;00m [38;5;21minner[39m([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs):
[0;32m---> 36[0m     [38;5;28;01mreturn[39;00m [43mfn[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:487[0m, in [0;36mcheckpoint[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)[0m
[1;32m    482[0m     [38;5;28;01mif[39;00m context_fn [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m noop_context_fn [38;5;129;01mor[39;00m debug [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mFalse[39;00m:
[1;32m    483[0m         [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    484[0m             [38;5;124m"[39m[38;5;124mPassing `context_fn` or `debug` is only supported when [39m[38;5;124m"[39m
[1;32m    485[0m             [38;5;124m"[39m[38;5;124muse_reentrant=False.[39m[38;5;124m"[39m
[1;32m    486[0m         )
[0;32m--> 487[0m     [38;5;28;01mreturn[39;00m [43mCheckpointFunction[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[43mfunction[49m[43m,[49m[43m [49m[43mpreserve[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    488[0m [38;5;28;01melse[39;00m:
[1;32m    489[0m     gen [38;5;241m=[39m _checkpoint_without_reentrant_generator(
[1;32m    490[0m         function, preserve, context_fn, determinism_check, debug, [38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs
[1;32m    491[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/autograd/function.py:598[0m, in [0;36mFunction.apply[0;34m(cls, *args, **kwargs)[0m
[1;32m    595[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m torch[38;5;241m.[39m_C[38;5;241m.[39m_are_functorch_transforms_active():
[1;32m    596[0m     [38;5;66;03m# See NOTE: [functorch vjp and autograd interaction][39;00m
[1;32m    597[0m     args [38;5;241m=[39m _functorch[38;5;241m.[39mutils[38;5;241m.[39munwrap_dead_wrappers(args)
[0;32m--> 598[0m     [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mapply[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m    600[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m is_setup_ctx_defined:
[1;32m    601[0m     [38;5;28;01mraise[39;00m [38;5;167;01mRuntimeError[39;00m(
[1;32m    602[0m         [38;5;124m"[39m[38;5;124mIn order to use an autograd.Function with functorch transforms [39m[38;5;124m"[39m
[1;32m    603[0m         [38;5;124m"[39m[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context [39m[38;5;124m"[39m
[1;32m    604[0m         [38;5;124m"[39m[38;5;124mstaticmethod. For more details, please see [39m[38;5;124m"[39m
[1;32m    605[0m         [38;5;124m"[39m[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html[39m[38;5;124m"[39m
[1;32m    606[0m     )

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:262[0m, in [0;36mCheckpointFunction.forward[0;34m(ctx, run_function, preserve_rng_state, *args)[0m
[1;32m    259[0m ctx[38;5;241m.[39msave_for_backward([38;5;241m*[39mtensor_inputs)
[1;32m    261[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
[0;32m--> 262[0m     outputs [38;5;241m=[39m [43mrun_function[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m)[49m
[1;32m    263[0m [38;5;28;01mreturn[39;00m outputs

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:816[0m, in [0;36mLlamaModel_fast_forward.<locals>.create_custom_forward.<locals>.custom_forward[0;34m(*inputs)[0m
[1;32m    815[0m [38;5;28;01mdef[39;00m [38;5;21mcustom_forward[39m([38;5;241m*[39minputs):
[0;32m--> 816[0m     [38;5;28;01mreturn[39;00m [43mmodule[49m[43m([49m[38;5;241;43m*[39;49m[43minputs[49m[43m,[49m[43m [49m[43mpast_key_value[49m[43m,[49m[43m [49m[43moutput_attentions[49m[43m,[49m[43m [49m[43mpadding_mask[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mpadding_mask[49m[43m,[49m[43m [49m[43mposition_embeddings[49m[43m [49m[38;5;241;43m=[39;49m[43m [49m[43mposition_embeddings[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:493[0m, in [0;36mLlamaDecoderLayer_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    491[0m residual [38;5;241m=[39m hidden_states
[1;32m    492[0m hidden_states [38;5;241m=[39m fast_rms_layernorm([38;5;28mself[39m[38;5;241m.[39minput_layernorm, hidden_states)
[0;32m--> 493[0m hidden_states, self_attn_weights, present_key_value [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mself_attn[49m[43m([49m
[1;32m    494[0m [43m    [49m[43mhidden_states[49m[38;5;241;43m=[39;49m[43mhidden_states[49m[43m,[49m
[1;32m    495[0m [43m    [49m[43mcausal_mask[49m[38;5;241;43m=[39;49m[43mcausal_mask[49m[43m,[49m
[1;32m    496[0m [43m    [49m[43mattention_mask[49m[38;5;241;43m=[39;49m[43mattention_mask[49m[43m,[49m
[1;32m    497[0m [43m    [49m[43mposition_ids[49m[38;5;241;43m=[39;49m[43mposition_ids[49m[43m,[49m
[1;32m    498[0m [43m    [49m[43mpast_key_value[49m[38;5;241;43m=[39;49m[43mpast_key_value[49m[43m,[49m
[1;32m    499[0m [43m    [49m[43moutput_attentions[49m[38;5;241;43m=[39;49m[43moutput_attentions[49m[43m,[49m
[1;32m    500[0m [43m    [49m[43muse_cache[49m[38;5;241;43m=[39;49m[43muse_cache[49m[43m,[49m
[1;32m    501[0m [43m    [49m[43mpadding_mask[49m[38;5;241;43m=[39;49m[43mpadding_mask[49m[43m,[49m
[1;32m    502[0m [43m[49m[43m)[49m
[1;32m    503[0m hidden_states [38;5;241m=[39m residual [38;5;241m+[39m hidden_states
[1;32m    505[0m [38;5;66;03m# Fully Connected[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532[0m, in [0;36mModule._wrapped_call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1530[0m     [38;5;28;01mreturn[39;00m [38;5;28mself[39m[38;5;241m.[39m_compiled_call_impl([38;5;241m*[39margs, [38;5;241m*[39m[38;5;241m*[39mkwargs)  [38;5;66;03m# type: ignore[misc][39;00m
[1;32m   1531[0m [38;5;28;01melse[39;00m:
[0;32m-> 1532[0m     [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_call_impl[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541[0m, in [0;36mModule._call_impl[0;34m(self, *args, **kwargs)[0m
[1;32m   1536[0m [38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in[39;00m
[1;32m   1537[0m [38;5;66;03m# this function, and just call forward.[39;00m
[1;32m   1538[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m ([38;5;28mself[39m[38;5;241m.[39m_backward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_backward_pre_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_hooks [38;5;129;01mor[39;00m [38;5;28mself[39m[38;5;241m.[39m_forward_pre_hooks
[1;32m   1539[0m         [38;5;129;01mor[39;00m _global_backward_pre_hooks [38;5;129;01mor[39;00m _global_backward_hooks
[1;32m   1540[0m         [38;5;129;01mor[39;00m _global_forward_hooks [38;5;129;01mor[39;00m _global_forward_pre_hooks):
[0;32m-> 1541[0m     [38;5;28;01mreturn[39;00m [43mforward_call[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m   1543[0m [38;5;28;01mtry[39;00m:
[1;32m   1544[0m     result [38;5;241m=[39m [38;5;28;01mNone[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/unsloth/models/llama.py:372[0m, in [0;36mLlamaAttention_fast_forward[0;34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, *args, **kwargs)[0m
[1;32m    369[0m     kv_seq_len [38;5;241m+[39m[38;5;241m=[39m past_key_value[[38;5;241m0[39m][38;5;241m.[39mshape[[38;5;241m-[39m[38;5;241m2[39m]
[1;32m    371[0m [38;5;66;03m# Extend RoPE dynamically to fit in VRAM[39;00m
[0;32m--> 372[0m rotary_emb [38;5;241m=[39m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mrotary_emb[49m
[1;32m    373[0m rotary_emb[38;5;241m.[39mextend_rope_embedding(V, seq_len [38;5;241m=[39m kv_seq_len)
[1;32m    375[0m [38;5;28;01mif[39;00m position_ids [38;5;129;01mis[39;00m [38;5;28;01mNone[39;00m:
[1;32m    376[0m     [38;5;66;03m# Useful for LongRoPE[39;00m

File [0;32m~/projects/arc-challenge/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1709[0m, in [0;36mModule.__getattr__[0;34m(self, name)[0m
[1;32m   1707[0m     [38;5;28;01mif[39;00m name [38;5;129;01min[39;00m modules:
[1;32m   1708[0m         [38;5;28;01mreturn[39;00m modules[name]
[0;32m-> 1709[0m [38;5;28;01mraise[39;00m [38;5;167;01mAttributeError[39;00m([38;5;124mf[39m[38;5;124m"[39m[38;5;124m'[39m[38;5;132;01m{[39;00m[38;5;28mtype[39m([38;5;28mself[39m)[38;5;241m.[39m[38;5;18m__name__[39m[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m object has no attribute [39m[38;5;124m'[39m[38;5;132;01m{[39;00mname[38;5;132;01m}[39;00m[38;5;124m'[39m[38;5;124m"[39m)

[0;31mAttributeError[0m: 'LlamaAttention' object has no attribute 'rotary_emb'


PeftModelForCausalLM(
  (base_model): LoraModel(
    (model): LlamaForCausalLM(
      (model): LlamaModel(
        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)
        (layers): ModuleList(
          (0-31): 32 x LlamaDecoderLayer(
            (self_attn): LlamaAttention(
              (q_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (k_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (v_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=1024, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (o_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
            )
            (mlp): LlamaMLP(
              (gate_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (up_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=4096, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=14336, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (down_proj): lora.Linear4bit(
                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Identity()
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=14336, out_features=64, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=64, out_features=4096, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict()
              )
              (act_fn): SiLU()
            )
            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
          )
        )
        (norm): LlamaRMSNorm((4096,), eps=1e-05)
        (rotary_emb): LlamaRotaryEmbedding()
      )
      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
    )
  )
)
